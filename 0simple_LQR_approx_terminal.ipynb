{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mpc import mpc\n",
    "from mpc import util\n",
    "import torch.nn.functional as F\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost net (simple running cost and MLP terminal cost)\n",
    "g = x^2 + u^2\n",
    "\n",
    "\n",
    "F = xT * MLP(x) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCCostNetwork(nn.Module):\n",
    "    def __init__(self, n_state, n_ctrl):\n",
    "        super().__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_ctrl = n_ctrl\n",
    "\n",
    "        # 运行损失 xTQx + uTRu\n",
    "        self.Q = torch.eye(n_state)\n",
    "        self.R = torch.eye(n_ctrl)\n",
    "\n",
    "        # 终端损失网络 xTFx\n",
    "        self.terminal_net = nn.Sequential(\n",
    "            nn.Linear(n_state, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_state * n_state)\n",
    "        )\n",
    "        \n",
    "    def forward(self, tau, t=None, T=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tau: [batch_size, n_state + n_ctrl], concatenated form of state and action\n",
    "            t: Current timestamp\n",
    "            T: Total prediction horizon\n",
    "        Returns:\n",
    "            cost: [batch_size] \n",
    "        \"\"\"\n",
    "        batch_size = tau.size(0)\n",
    "        \n",
    "        # 分离状态和控制\n",
    "        # 如果t == T-1，ctrl将会是是零向量，表示不存在，所以不会index out of range\n",
    "        state = tau[:, :self.n_state]\n",
    "        ctrl = tau[:, self.n_state:]\n",
    "        \n",
    "        # 如果是最后一个时间步，只计算终端损失\n",
    "        # if t is not None and T is not None and t == T-1:\n",
    "        #     terminal_F = self.terminal_net(state).view(batch_size, self.n_state, self.n_state)\n",
    "        #     state = state.unsqueeze(1)\n",
    "        #     terminal_cost = torch.bmm(torch.bmm(state, terminal_F), state.transpose(1, 2))\n",
    "        #     # pdb.set_trace()\n",
    "        #     return terminal_cost.squeeze(-1).squeeze(-1) # Return shape: (batch_size, )\n",
    "        # else:\n",
    "        #     return self.running_cost(state, ctrl)\n",
    "        return self.running_cost(state, ctrl)\n",
    "    \n",
    "    def running_cost(self, state, ctrl): \n",
    "        # 简单二次型运行代价\n",
    "        batch_size = state.shape[0]\n",
    "        state = state.unsqueeze(1) # (batch_size, 1, n_state)\n",
    "        ctrl = ctrl.unsqueeze(1)\n",
    "        Q = self.Q.repeat(batch_size, 1, 1) # (batch_size, n_state, n_state)\n",
    "        R = self.R.repeat(batch_size, 1, 1)\n",
    "        xTQx = torch.bmm(torch.bmm(state, Q), state.transpose(1, 2))\n",
    "        uTRx = torch.bmm(torch.bmm(ctrl, R), ctrl.transpose(1, 2))\n",
    "        return (xTQx + uTRx).squeeze(-1).squeeze(-1) # Return shape: (batch_size,)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bolza to Lagrange, so that the cost function can fit the mpc api.\n",
    "class MPCCostNetwork_Lagrange(nn.Module):\n",
    "    def __init__(self, n_state, n_ctrl):\n",
    "        super().__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_ctrl = n_ctrl\n",
    "\n",
    "        # 运行损失 xTQx + uTRu\n",
    "        self.Q = torch.eye(n_state)\n",
    "        self.R = torch.eye(n_ctrl)\n",
    "\n",
    "        # 终端损失网络 xTFx\n",
    "        self.terminal_net = nn.Sequential(\n",
    "            nn.Linear(n_state, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_state * n_state)\n",
    "        )\n",
    "        \n",
    "    def forward(self, tau, last_tau=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tau: [batch_size, n_state + n_ctrl], concatenated form of state and action\n",
    "            t: Current timestamp\n",
    "            T: Total prediction horizon\n",
    "        Returns:\n",
    "            cost: [batch_size] \n",
    "        \"\"\"\n",
    "        batch_size = tau.size(0)\n",
    "        \n",
    "        # 分离状态和控制\n",
    "        # 如果t == T-1，ctrl将会是是零向量，表示不存在，所以不会index out of range\n",
    "        state = tau[:, :self.n_state]\n",
    "        ctrl = tau[:, self.n_state:]\n",
    "        state_last = last_tau[:, :self.n_state]\n",
    "        ctrl_last = last_tau[:, self.n_state:]\n",
    "        \n",
    "        return self.running_cost(state, ctrl) + self.terminal_bias(state, state_last)\n",
    "    \n",
    "    def running_cost(self, state, ctrl): \n",
    "        # 简单二次型运行代价\n",
    "        batch_size = state.shape[0]\n",
    "        state = state.unsqueeze(1) # (batch_size, 1, n_state)\n",
    "        ctrl = ctrl.unsqueeze(1)\n",
    "        Q = self.Q.repeat(batch_size, 1, 1) # (batch_size, n_state, n_state)\n",
    "        R = self.R.repeat(batch_size, 1, 1)\n",
    "        xTQx = torch.bmm(torch.bmm(state, Q), state.transpose(1, 2))\n",
    "        uTRx = torch.bmm(torch.bmm(ctrl, R), ctrl.transpose(1, 2))\n",
    "        return (xTQx + uTRx).squeeze(-1).squeeze(-1) # Return shape: (batch_size,)\n",
    "    \n",
    "    def terminal_bias(self, state, last_state):\n",
    "        x_detached = state.detach()\n",
    "        term_cost = self.terminal_net(x_detached)\n",
    "        term_cost.backward()\n",
    "        dhdx = x_detached.grad # Shape: (batch_size, n_state)\n",
    "        dxdt = state - last_state # Shape: (batch_size, n_state)\n",
    "        print(dhdx.unsqueeze(1).shape, dxdt.unsqueeze(-1).shape)\n",
    "        # dh/dx * dx/dt\n",
    "        term_bias = torch.bmm(dhdx.unsqueeze(1), (state - last_state).unsqueeze(-1)).squeeze(-1).squeeze(-1)\n",
    "        return term_bias\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sys Dyna (F = x + u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsF(nn.Module):\n",
    "    def forward(self, state, action):\n",
    "        # Dimension of state & action: (batch_size, C)\n",
    "        assert state.shape[-1] == action.shape[-1]\n",
    "        return state + action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the original MPC lib\n",
    "To support terminal cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "class myMPC(mpc.MPC):\n",
    "    def approximate_cost(self, x, u, Cf, diff=True):\n",
    "        with torch.enable_grad():\n",
    "            tau = torch.cat((x, u), dim=2).data\n",
    "            tau = Variable(tau, requires_grad=True)\n",
    "\n",
    "            if self.slew_rate_penalty is not None:\n",
    "                print(\"\"\"\n",
    "                MPC Error: Using a non-convex cost with a slew rate penalty is not yet implemented.\n",
    "                The current implementation does not correctly do a line search.\n",
    "                More details: https://github.com/locuslab/mpc.pytorch/issues/12\n",
    "                \"\"\")\n",
    "                sys.exit(-1)\n",
    "            \n",
    "            costs = list()\n",
    "            hessians = list()\n",
    "            grads = list()\n",
    "            for t in range(self.T):\n",
    "                tau_t = tau[t]\n",
    "                cost = Cf(tau_t, t=t, T=self.T) #\n",
    "                grad = torch.autograd.grad(cost.sum(), tau_t,\n",
    "                                           create_graph=True, retain_graph=True)[0]\n",
    "                hessian = list()\n",
    "                for v_i in range(tau.shape[2]):\n",
    "                    hessian.append(\n",
    "                        torch.autograd.grad(grad[:, v_i].sum(), tau_t,\n",
    "                                            retain_graph=True)[0]\n",
    "                    )\n",
    "                hessian = torch.stack(hessian, dim=-1)\n",
    "                costs.append(cost)\n",
    "                grads.append(grad - util.bmv(hessian, tau_t))\n",
    "                hessians.append(hessian)\n",
    "            \n",
    "            costs = torch.stack(costs, dim=0)\n",
    "            grads = torch.stack(grads, dim=0)\n",
    "            hessians = torch.stack(hessians, dim=0)\n",
    "            if not diff:\n",
    "                return hessians.data, grads.data, costs.data\n",
    "            return hessians, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve optimization problem use MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[0;32m     26\u001b[0m     ctrl \u001b[38;5;241m=\u001b[39m LagrangeMPC(nx, nu, mpc_T, lqr_iter\u001b[38;5;241m=\u001b[39mLQR_ITER, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     27\u001b[0m                 exit_unconverged\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, n_batch\u001b[38;5;241m=\u001b[39mbatch_size, backprop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, u_init\u001b[38;5;241m=\u001b[39mu_init,\n\u001b[0;32m     28\u001b[0m                 grad_method\u001b[38;5;241m=\u001b[39mmpc\u001b[38;5;241m.\u001b[39mGradMethods\u001b[38;5;241m.\u001b[39mAUTO_DIFF)\n\u001b[1;32m---> 30\u001b[0m     x_seq, u_seq, objs \u001b[38;5;241m=\u001b[39m \u001b[43mctrl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_now\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m u_seq[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     33\u001b[0m     VN_list\u001b[38;5;241m.\u001b[39mappend(objs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\graduation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\graduation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\graduation\\lib\\site-packages\\mpc\\mpc.py:242\u001b[0m, in \u001b[0;36mMPC.forward\u001b[1;34m(self, x_init, cost, dx)\u001b[0m\n\u001b[0;32m    238\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mtype_as(x_init\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitial mean(cost): \u001b[39m\u001b[38;5;132;01m{:.4e}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m--> 242\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmean(\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_init\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    245\u001b[0m     ))\n\u001b[0;32m    247\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m n_not_improved \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\graduation\\lib\\site-packages\\mpc\\util.py:149\u001b[0m, in \u001b[0;36mget_cost\u001b[1;34m(T, u, cost, dynamics, x_init, x)\u001b[0m\n\u001b[0;32m    147\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mbquad(xut, C[t]) \u001b[38;5;241m+\u001b[39m bdot(xut, c[t])\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mcost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxut\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     objs\u001b[38;5;241m.\u001b[39mappend(obj)\n\u001b[0;32m    151\u001b[0m objs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(objs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\graduation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\graduation\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m, in \u001b[0;36mMPCCostNetwork_Lagrange.forward\u001b[1;34m(self, tau, last_tau)\u001b[0m\n\u001b[0;32m     34\u001b[0m state \u001b[38;5;241m=\u001b[39m tau[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_state]\n\u001b[0;32m     35\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m tau[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_state:]\n\u001b[1;32m---> 36\u001b[0m state_last \u001b[38;5;241m=\u001b[39m \u001b[43mlast_tau\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_state\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     37\u001b[0m ctrl_last \u001b[38;5;241m=\u001b[39m last_tau[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_state:]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_cost(state, ctrl) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal_bias(state, state_last)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from mpc.mpc import QuadCost\n",
    "\n",
    "LQR_ITER = 100\n",
    "batch_size, T, mpc_T = 1, 5, 5\n",
    "DTYPE = torch.float\n",
    "nx, nu = 1, 1 # 用1, 1会出问题，最后得到的控制都是nan\n",
    "n_sc = nx + nu\n",
    "dynamics = DynamicsF()\n",
    "cost = MPCCostNetwork_Lagrange(nx, nu)\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "init_state = torch.randn(batch_size, nx, dtype=DTYPE)\n",
    "u_init = None\n",
    "x_now = init_state\n",
    "\n",
    "C = torch.randn(T*batch_size, n_sc, n_sc) # shape (T*n_batch, n_sc, n_sc)\n",
    "C = torch.bmm(C, C.transpose(1, 2)).view(T, batch_size, n_sc, n_sc) # shape (T, n_batch, n_sc, n_sc) # 二次项损失\n",
    "c = torch.randn(T, batch_size, n_sc) # 一次项损失\n",
    "Qd_cost = QuadCost(C, c)\n",
    "\n",
    "VN_list = []\n",
    "x_list = []\n",
    "u_list = []\n",
    "for t in range(T):\n",
    "    ctrl = LagrangeMPC(nx, nu, mpc_T, lqr_iter=LQR_ITER, verbose=1,\n",
    "                exit_unconverged=False, eps=1e-2, n_batch=batch_size, backprop=False, u_init=u_init,\n",
    "                grad_method=mpc.GradMethods.AUTO_DIFF)\n",
    "\n",
    "    x_seq, u_seq, objs = ctrl(x_now, cost, dynamics)\n",
    "    action = u_seq[0]\n",
    "\n",
    "    VN_list.append(objs)\n",
    "    x_list.append(x_now)\n",
    "    u_list.append(action)\n",
    "\n",
    "    x_now = dynamics(x_now, action)\n",
    "    u_init = torch.cat((u_seq[1:], torch.zeros(1, batch_size, nu, dtype=DTYPE)), dim=0)\n",
    "    # print(x_seq) # Shape: (mpc_T, batch_size, nx)\n",
    "    # print(u_seq) # Shape: (mpc_T, batch_size, nu)\n",
    "    # print(objs)  # Shape: (batch_size,)\n",
    "    # Verbose > 0时，打印出来的log数据分别表示：\n",
    "    # iLQR迭代次数、最优轨迹的平均代价、action更新的最大范数(表示action的变化量)、线搜索步长均值、QP子问题的总迭代次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the RDP inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VN_list)\n",
    "print(x_list)\n",
    "print(u_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_RDP_ineq(VN_list, x_list, u_list, cost, alpha):\n",
    "    RDP_ineq = []\n",
    "    for t in range(T-1):\n",
    "        tau = torch.cat((x_list[t], u_list[t]), dim=-1)\n",
    "        RDP = VN_list[t] - (VN_list[t+1] + alpha * cost(tau, t, T))\n",
    "        RDP_ineq.append(RDP)\n",
    "    return RDP_ineq\n",
    "\n",
    "RDP_ineq = cal_RDP_ineq(VN_list, x_list, u_list, cost, 1)\n",
    "print(RDP_ineq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interior_point_loss(RDP_ineq):\n",
    "    loss = 0\n",
    "    for RDP in RDP_ineq:\n",
    "        loss += -torch.log(RDP)\n",
    "    return loss\n",
    "\n",
    "loss = interior_point_loss(RDP_ineq)\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(T-1):\n",
    "    tau = torch.cat((x_list[t], u_list[t]), dim=-1)\n",
    "    RDP_ineq = VN_list[t+1] + alpha * cost(tau, t, T) - VN_list[t]\n",
    "    print(RDP_ineq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graduation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
